"""
End-to-end integration tests for the Code Review & Documentation Agent.

This module tests the complete workflow including:
- Multi-agent coordination
- Pause/resume functionality
- Memory Bank persistence across sessions
- API endpoints with various scenarios
- Webhook notifications
- CI/CD integration

Task 19: Final integration and end-to-end testing
"""

import pytest
import tempfile
import shutil
import time
import json
from pathlib import Path
from datetime import datetime, timezone
from unittest.mock import Mock, patch, AsyncMock
from fastapi.testclient import TestClient

from agents.coordinator_agent import CoordinatorAgent
from storage.memory_bank import MemoryBank
from storage.session_manager import SessionManager
from models.data_models import (
    AnalysisConfig,
    AnalysisDepth,
    SessionStatus,
    ProjectPattern,
    PatternType,
)
from api.main import app


# ============================================================================
# Test Fixtures
# ============================================================================

@pytest.fixture
def temp_dir():
    """Create a temporary directory for testing."""
    temp_path = tempfile.mkdtemp()
    yield temp_path
    shutil.rmtree(temp_path, ignore_errors=True)


@pytest.fixture
def coordinator(temp_dir):
    """Create a coordinator agent with temporary storage."""
    memory_bank = MemoryBank(db_path=str(Path(temp_dir) / "test_memory.db"))
    session_manager = SessionManager(sessions_dir=str(Path(temp_dir) / "sessions"))
    return CoordinatorAgent(memory_bank=memory_bank, session_manager=session_manager)


@pytest.fixture
def api_client():
    """Create a test client for the API."""
    return TestClient(app)


@pytest.fixture
def small_codebase(temp_dir):
    """Create a small sample codebase (5 files)."""
    codebase_path = Path(temp_dir) / "small_codebase"
    codebase_path.mkdir()
    
    # Create Python files with varying complexity
    (codebase_path / "simple.py").write_text("""
def add(a, b):
    \"\"\"Add two numbers.\"\"\"
    return a + b

def multiply(a, b):
    \"\"\"Multiply two numbers.\"\"\"
    return a * b
""")
    
    (codebase_path / "moderate.py").write_text("""
class Calculator:
    \"\"\"A simple calculator class.\"\"\"
    
    def __init__(self):
        self.history = []
    
    def add(self, a, b):
        result = a + b
        self.history.append(f"{a} + {b} = {result}")
        return result
    
    def divide(self, a, b):
        if b == 0:
            raise ValueError("Cannot divide by zero")
        result = a / b
        self.history.append(f"{a} / {b} = {result}")
        return result
""")
    
    (codebase_path / "complex.py").write_text("""
def complex_function(x, y, z):
    \"\"\"A function with higher complexity.\"\"\"
    if x > 0:
        if y > 0:
            if z > 0:
                return x + y + z
            else:
                return x + y - z
        else:
            if z > 0:
                return x - y + z
            else:
                return x - y - z
    else:
        if y > 0:
            if z > 0:
                return -x + y + z
            else:
                return -x + y - z
        else:
            return -x - y - z
""")
    
    (codebase_path / "utils.py").write_text("""
import os
import sys

def read_file(path):
    \"\"\"Read a file without error handling.\"\"\"
    with open(path) as f:
        return f.read()

def write_file(path, content):
    \"\"\"Write to a file.\"\"\"
    with open(path, 'w') as f:
        f.write(content)
""")
    
    (codebase_path / "main.py").write_text("""
from simple import add, multiply
from moderate import Calculator

def main():
    \"\"\"Main entry point.\"\"\"
    calc = Calculator()
    result1 = calc.add(5, 3)
    result2 = calc.divide(10, 2)
    print(f"Results: {result1}, {result2}")

if __name__ == "__main__":
    main()
""")
    
    return str(codebase_path)


@pytest.fixture
def medium_codebase(temp_dir):
    """Create a medium sample codebase (20 files)."""
    codebase_path = Path(temp_dir) / "medium_codebase"
    codebase_path.mkdir()
    
    # Create multiple modules
    for i in range(20):
        (codebase_path / f"module_{i}.py").write_text(f"""
def function_{i}_a(x):
    \"\"\"Function {i}a.\"\"\"
    return x * {i}

def function_{i}_b(x, y):
    \"\"\"Function {i}b.\"\"\"
    if x > y:
        return x - y
    else:
        return y - x

class Class_{i}:
    \"\"\"Class {i}.\"\"\"
    
    def __init__(self, value):
        self.value = value
    
    def process(self):
        return self.value * {i}
""")
    
    return str(codebase_path)


@pytest.fixture
def large_codebase(temp_dir):
    """Create a large sample codebase (100 files)."""
    codebase_path = Path(temp_dir) / "large_codebase"
    codebase_path.mkdir()
    
    # Create multiple directories with files
    for dir_idx in range(10):
        dir_path = codebase_path / f"package_{dir_idx}"
        dir_path.mkdir()
        
        for file_idx in range(10):
            (dir_path / f"module_{file_idx}.py").write_text(f"""
def func_{dir_idx}_{file_idx}(x, y):
    \"\"\"Function {dir_idx}_{file_idx}.\"\"\"
    result = x + y + {dir_idx} + {file_idx}
    return result

class Component_{dir_idx}_{file_idx}:
    \"\"\"Component {dir_idx}_{file_idx}.\"\"\"
    
    def __init__(self):
        self.id = {dir_idx * 10 + file_idx}
    
    def execute(self):
        return self.id * 2
""")
    
    return str(codebase_path)


# ============================================================================
# End-to-End Tests: Complete Workflow on Sample Codebases
# ============================================================================

def test_e2e_small_codebase_analysis(coordinator, small_codebase):
    """
    Test complete workflow on a small codebase (5 files).
    
    Validates:
    - File discovery
    - Code parsing
    - Analysis execution
    - Report generation
    - Quality score calculation
    """
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    # Run analysis
    result = coordinator.analyze_codebase(config)
    
    # Verify basic results
    assert result is not None
    assert result.session_id is not None
    assert result.files_analyzed == 5
    assert 0 <= result.quality_score <= 100
    assert len(result.file_analyses) == 5
    
    # Verify documentation was generated
    assert result.documentation is not None
    assert len(result.documentation.project_structure) > 0
    
    # Verify metrics summary
    assert result.metrics_summary.total_files == 5
    assert result.metrics_summary.total_lines > 0
    assert result.metrics_summary.average_complexity >= 0
    
    # Verify report can be generated
    report = coordinator.generate_review_report(result)
    assert len(report) > 0
    assert "Code Review Report" in report or "code review" in report.lower()


def test_e2e_medium_codebase_analysis(coordinator, medium_codebase):
    """
    Test complete workflow on a medium codebase (20 files).
    
    Validates:
    - Parallel processing
    - Scalability
    - Performance
    """
    config = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK,
        enable_parallel=True
    )
    
    start_time = time.time()
    result = coordinator.analyze_codebase(config)
    duration = time.time() - start_time
    
    # Verify results
    assert result.files_analyzed == 20
    assert len(result.file_analyses) == 20
    assert 0 <= result.quality_score <= 100
    
    # Verify reasonable performance (should complete in reasonable time)
    assert duration < 60, f"Analysis took {duration}s, expected < 60s"
    
    # Verify all files were analyzed
    analyzed_files = {fa.file_path for fa in result.file_analyses}
    assert len(analyzed_files) == 20


def test_e2e_large_codebase_analysis(coordinator, large_codebase):
    """
    Test complete workflow on a large codebase (100 files).
    
    Validates:
    - Large-scale processing
    - Memory efficiency
    - Parallel execution benefits
    """
    config = AnalysisConfig(
        target_path=large_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK,
        enable_parallel=True
    )
    
    start_time = time.time()
    result = coordinator.analyze_codebase(config)
    duration = time.time() - start_time
    
    # Verify results
    assert result.files_analyzed == 100
    assert len(result.file_analyses) == 100
    assert 0 <= result.quality_score <= 100
    
    # Verify reasonable performance
    assert duration < 180, f"Analysis took {duration}s, expected < 180s"
    
    # Verify metrics summary
    assert result.metrics_summary.total_files == 100
    assert result.metrics_summary.total_lines > 0


# ============================================================================
# End-to-End Tests: Multi-Agent Coordination
# ============================================================================

def test_e2e_multi_agent_coordination(coordinator, small_codebase):
    """
    Test that all agents work together correctly.
    
    Validates:
    - Analyzer agent execution
    - Documenter agent execution
    - Reviewer agent execution
    - Result aggregation
    """
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    result = coordinator.analyze_codebase(config)
    
    # Verify analyzer results (file analyses with metrics and issues)
    assert len(result.file_analyses) > 0
    for fa in result.file_analyses:
        assert fa.metrics is not None
        assert fa.metrics.cyclomatic_complexity >= 0
        assert 0 <= fa.metrics.maintainability_index <= 100
    
    # Verify documenter results (documentation generated)
    assert result.documentation is not None
    assert len(result.documentation.project_structure) > 0
    
    # Verify reviewer results (suggestions generated)
    # Note: Suggestions may be empty if no issues found
    assert isinstance(result.suggestions, list)
    
    # Verify coordinator aggregated everything
    assert result.metrics_summary is not None
    assert result.quality_score is not None


def test_e2e_parallel_vs_sequential_execution(coordinator, medium_codebase):
    """
    Test that parallel execution is faster than sequential.
    
    Validates:
    - Parallel processing works
    - Performance improvement
    """
    # Run with parallel execution
    config_parallel = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK,
        enable_parallel=True
    )
    
    start_parallel = time.time()
    result_parallel = coordinator.analyze_codebase(config_parallel)
    duration_parallel = time.time() - start_parallel
    
    # Run with sequential execution
    config_sequential = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK,
        enable_parallel=False
    )
    
    start_sequential = time.time()
    result_sequential = coordinator.analyze_codebase(config_sequential)
    duration_sequential = time.time() - start_sequential
    
    # Verify both produced same number of results
    assert result_parallel.files_analyzed == result_sequential.files_analyzed
    
    # Verify parallel is faster (or at least not significantly slower)
    # Allow significant variance due to overhead, especially for small codebases
    # Parallel processing has overhead that may not be worth it for small codebases
    assert duration_parallel <= duration_sequential * 2.5, \
        f"Parallel ({duration_parallel}s) should not be significantly slower than sequential ({duration_sequential}s)"


# ============================================================================
# End-to-End Tests: Pause/Resume Functionality
# ============================================================================

def test_e2e_pause_resume_workflow(coordinator, medium_codebase):
    """
    Test complete pause and resume workflow.
    
    Validates:
    - Session creation
    - Pause operation
    - State persistence
    - Resume operation
    - Completion
    """
    config = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    # Start analysis with specific session ID
    session_id = "test_pause_resume_e2e"
    
    # Create session manually to control the process
    files = coordinator.file_system.discover_files(
        medium_codebase,
        include_patterns=["*.py"],
        exclude_patterns=[]
    )
    
    session_state = coordinator.session_manager.create_session(
        session_id=session_id,
        config=config,
        pending_files=files
    )
    
    assert session_state.status == SessionStatus.RUNNING
    
    # Simulate partial processing
    processed_count = len(files) // 2
    processed_files = files[:processed_count]
    pending_files = files[processed_count:]
    
    # Create mock analyses for processed files
    from models.data_models import FileAnalysis, CodeMetrics
    mock_analyses = []
    for file_path in processed_files:
        mock_analysis = FileAnalysis(
            file_path=file_path,
            language="python",
            metrics=CodeMetrics(
                cyclomatic_complexity=2,
                maintainability_index=90.0,
                lines_of_code=20,
                comment_ratio=0.2
            ),
            issues=[],
            functions=[],
            classes=[]
        )
        mock_analyses.append(mock_analysis.model_dump(mode='json'))
    
    coordinator.session_manager.checkpoint(
        session_id=session_id,
        processed_files=processed_files,
        pending_files=pending_files,
        partial_results={
            'analysis_count': processed_count,
            'file_analyses': mock_analyses
        }
    )
    
    # Pause the analysis
    pause_success = coordinator.pause_analysis(session_id)
    assert pause_success is True
    
    # Verify session is paused
    paused_state = coordinator.get_analysis_status(session_id)
    assert paused_state.status == SessionStatus.PAUSED
    assert len(paused_state.processed_files) == processed_count
    assert len(paused_state.pending_files) == len(files) - processed_count
    
    # Resume the analysis
    result = coordinator.resume_analysis(session_id)
    
    # Verify completion
    assert result is not None
    assert result.session_id == session_id
    assert result.files_analyzed == len(files)
    
    # Verify final session state
    final_state = coordinator.get_analysis_status(session_id)
    assert final_state.status == SessionStatus.COMPLETED


def test_e2e_pause_resume_with_file_changes(coordinator, small_codebase):
    """
    Test pause/resume with file modifications during pause.
    
    Validates:
    - Change detection
    - Re-analysis of modified files
    - Preservation of unmodified file results
    """
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    session_id = "test_pause_resume_changes"
    
    # Create session
    files = coordinator.file_system.discover_files(
        small_codebase,
        include_patterns=["*.py"],
        exclude_patterns=[]
    )
    
    session_state = coordinator.session_manager.create_session(
        session_id=session_id,
        config=config,
        pending_files=files
    )
    
    # Process some files
    processed_count = max(1, len(files) // 2)
    processed_files = files[:processed_count]
    pending_files = files[processed_count:]
    
    from models.data_models import FileAnalysis, CodeMetrics
    mock_analyses = []
    for file_path in processed_files:
        mock_analysis = FileAnalysis(
            file_path=file_path,
            language="python",
            metrics=CodeMetrics(
                cyclomatic_complexity=1,
                maintainability_index=95.0,
                lines_of_code=10,
                comment_ratio=0.2
            ),
            issues=[],
            functions=[],
            classes=[]
        )
        mock_analyses.append(mock_analysis.model_dump(mode='json'))
    
    coordinator.session_manager.checkpoint(
        session_id=session_id,
        processed_files=processed_files,
        pending_files=pending_files,
        partial_results={
            'analysis_count': processed_count,
            'file_analyses': mock_analyses
        }
    )
    
    # Pause
    coordinator.pause_analysis(session_id)
    
    # Wait and modify a file
    time.sleep(0.1)
    if processed_files:
        modified_file = Path(processed_files[0])
        modified_file.write_text("""
def modified_function(x, y, z):
    \"\"\"This function was modified during pause.\"\"\"
    if x > 0:
        if y > 0:
            return x + y + z
        else:
            return x + y - z
    else:
        return x - y - z
""")
        time.sleep(0.1)
    
    # Resume
    result = coordinator.resume_analysis(session_id)
    
    # Verify all files were analyzed
    assert result.files_analyzed == len(files)
    assert len(result.file_analyses) == len(files)
    
    # Verify the modified file is in the results
    if processed_files:
        modified_path = processed_files[0]
        modified_analysis = next(
            (fa for fa in result.file_analyses if fa.file_path == modified_path),
            None
        )
        assert modified_analysis is not None


# ============================================================================
# End-to-End Tests: Memory Bank Persistence Across Sessions
# ============================================================================

def test_e2e_memory_bank_persistence(coordinator, small_codebase):
    """
    Test Memory Bank persistence across multiple analysis sessions.
    
    Validates:
    - Pattern storage
    - Pattern retrieval in subsequent sessions
    - Pattern updates based on feedback
    """
    project_id = "test_project_persistence"
    
    # First analysis session - store patterns
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    result1 = coordinator.analyze_codebase(config, session_id="session_1")
    
    # Store a pattern in Memory Bank
    pattern = ProjectPattern(
        pattern_id="naming_convention_1",
        project_id=project_id,
        pattern_type=PatternType.NAMING,
        description="Use snake_case for function names",
        examples=["def my_function():", "def calculate_total():"],
        confidence=0.9,
        last_updated=datetime.now(timezone.utc)
    )
    
    coordinator.memory_bank.store_pattern(pattern)
    
    # Second analysis session - retrieve patterns
    result2 = coordinator.analyze_codebase(config, session_id="session_2")
    
    # Retrieve the pattern
    retrieved_patterns = coordinator.memory_bank.retrieve_patterns(project_id)
    
    assert len(retrieved_patterns) > 0
    assert any(p.pattern_id == "naming_convention_1" for p in retrieved_patterns)
    
    # Verify pattern details
    retrieved_pattern = next(p for p in retrieved_patterns if p.pattern_id == "naming_convention_1")
    assert retrieved_pattern.description == pattern.description
    assert retrieved_pattern.confidence == pattern.confidence
    
    # Update pattern confidence based on feedback
    coordinator.memory_bank.update_pattern_confidence("naming_convention_1", feedback_positive=True)
    
    # Retrieve again and verify update
    updated_patterns = coordinator.memory_bank.retrieve_patterns(project_id)
    updated_pattern = next(p for p in updated_patterns if p.pattern_id == "naming_convention_1")
    assert updated_pattern.confidence > pattern.confidence


def test_e2e_memory_bank_multiple_projects(coordinator, small_codebase, medium_codebase):
    """
    Test Memory Bank with multiple projects.
    
    Validates:
    - Project isolation
    - Pattern retrieval by project
    """
    # Analyze first project
    config1 = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK
    )
    
    result1 = coordinator.analyze_codebase(config1)
    
    # Store patterns for project 1
    pattern1 = ProjectPattern(
        pattern_id="pattern_proj1",
        project_id="project_1",
        pattern_type=PatternType.STRUCTURE,
        description="Project 1 structure pattern",
        examples=["example1"],
        confidence=0.8,
        last_updated=datetime.now(timezone.utc)
    )
    coordinator.memory_bank.store_pattern(pattern1)
    
    # Analyze second project
    config2 = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK
    )
    
    result2 = coordinator.analyze_codebase(config2)
    
    # Store patterns for project 2
    pattern2 = ProjectPattern(
        pattern_id="pattern_proj2",
        project_id="project_2",
        pattern_type=PatternType.CONVENTION,
        description="Project 2 convention pattern",
        examples=["example2"],
        confidence=0.7,
        last_updated=datetime.now(timezone.utc)
    )
    coordinator.memory_bank.store_pattern(pattern2)
    
    # Retrieve patterns for each project
    patterns_proj1 = coordinator.memory_bank.retrieve_patterns("project_1")
    patterns_proj2 = coordinator.memory_bank.retrieve_patterns("project_2")
    
    # Verify isolation
    assert len(patterns_proj1) > 0
    assert len(patterns_proj2) > 0
    assert all(p.project_id == "project_1" for p in patterns_proj1)
    assert all(p.project_id == "project_2" for p in patterns_proj2)
    
    # Verify correct patterns retrieved
    assert any(p.pattern_id == "pattern_proj1" for p in patterns_proj1)
    assert any(p.pattern_id == "pattern_proj2" for p in patterns_proj2)
    assert not any(p.pattern_id == "pattern_proj2" for p in patterns_proj1)
    assert not any(p.pattern_id == "pattern_proj1" for p in patterns_proj2)


# ============================================================================
# End-to-End Tests: API Endpoints
# ============================================================================

def test_e2e_api_analyze_endpoint(api_client, small_codebase):
    """
    Test /analyze endpoint with complete workflow.
    
    Validates:
    - Request acceptance
    - Background processing
    - Status tracking
    - Result retrieval
    """
    with patch('api.main.run_analysis_async', new_callable=AsyncMock) as mock_run:
        mock_run.return_value = None
        
        # Trigger analysis
        response = api_client.post("/analyze", json={
            "codebase_path": small_codebase,
            "file_patterns": ["*.py"],
            "analysis_depth": "standard"
        })
        
        assert response.status_code == 200
        data = response.json()
        assert "session_id" in data
        assert "status" in data
        
        session_id = data["session_id"]
        
        # Check status
        status_response = api_client.get(f"/status/{session_id}")
        assert status_response.status_code in [200, 404]  # May not exist yet in test


def test_e2e_api_pause_resume_endpoints(api_client, temp_dir):
    """
    Test /pause and /resume endpoints.
    
    Validates:
    - Pause endpoint
    - Resume endpoint
    - State transitions
    """
    # Create a test session
    from storage.session_manager import SessionManager
    from models.data_models import SessionState, SessionStatus, AnalysisConfig, AnalysisDepth
    
    session_manager = SessionManager(sessions_dir=str(Path(temp_dir) / "sessions"))
    
    session_id = "test_api_pause_resume"
    config = AnalysisConfig(
        target_path=temp_dir,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    session_state = session_manager.create_session(
        session_id=session_id,
        config=config,
        pending_files=[]
    )
    
    # Test pause endpoint
    with patch('api.main.coordinator.pause_analysis', return_value=True):
        pause_response = api_client.post(f"/pause/{session_id}")
        # May return 404 if session not in API's coordinator
        assert pause_response.status_code in [200, 404]
    
    # Test resume endpoint
    with patch('api.main.coordinator.resume_analysis', return_value=None):
        resume_response = api_client.post(f"/resume/{session_id}")
        # May return 404 if session not in API's coordinator
        assert resume_response.status_code in [200, 404]


def test_e2e_api_history_endpoint(api_client):
    """
    Test /history endpoint.
    
    Validates:
    - History retrieval
    - Response format
    """
    response = api_client.get("/history")
    
    assert response.status_code == 200
    data = response.json()
    assert "analyses" in data
    assert "total" in data
    assert isinstance(data["analyses"], list)
    assert isinstance(data["total"], int)


def test_e2e_api_results_endpoint(api_client, temp_dir):
    """
    Test /results endpoint.
    
    Validates:
    - Results retrieval
    - JSON format
    - Required fields
    """
    # Create a completed session with results
    from storage.session_manager import SessionManager
    from models.data_models import (
        SessionState, SessionStatus, AnalysisConfig, AnalysisDepth,
        AnalysisResult, Documentation, MetricsSummary
    )
    
    session_manager = SessionManager(sessions_dir=str(Path(temp_dir) / "sessions"))
    
    session_id = "test_api_results"
    config = AnalysisConfig(
        target_path=temp_dir,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    # Create completed session
    session_state = SessionState(
        session_id=session_id,
        status=SessionStatus.COMPLETED,
        config=config,
        processed_files=[],
        pending_files=[],
        partial_results={
            "quality_score": 95.0,
            "files_analyzed": 5
        },
        checkpoint_time=datetime.now(timezone.utc)
    )
    
    session_manager.save_session(session_state)
    
    # Test results endpoint
    response = api_client.get(f"/results/{session_id}")
    
    # May return 404 if session not in API's coordinator
    if response.status_code == 200:
        data = response.json()
        assert "session_id" in data
        assert "status" in data
        assert "results" in data


# ============================================================================
# End-to-End Tests: Webhook Notifications
# ============================================================================

def test_e2e_webhook_notification_on_completion(coordinator, small_codebase):
    """
    Test webhook notification when analysis completes.
    
    Validates:
    - Webhook triggered on completion
    - Correct payload sent
    - HTTP POST to webhook URL
    """
    import asyncio
    from api.main import send_webhook_notification
    from models.data_models import AnalysisResult, Documentation, MetricsSummary
    
    webhook_url = "https://example.com/webhook"
    
    with patch('httpx.AsyncClient.post') as mock_post:
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response
        
        # Create test result
        result = AnalysisResult(
            session_id="test_webhook",
            timestamp=datetime.now(timezone.utc),
            codebase_path=small_codebase,
            files_analyzed=5,
            total_issues=2,
            quality_score=92.0,
            file_analyses=[],
            suggestions=[],
            documentation=Documentation(
                project_structure="Test",
                api_docs={},
                examples={}
            ),
            metrics_summary=MetricsSummary(
                total_files=5,
                total_lines=100,
                average_complexity=2.5,
                average_maintainability=92.0,
                total_issues_by_severity={},
                total_issues_by_category={}
            )
        )
        
        # Send webhook (run async function in sync context)
        asyncio.run(send_webhook_notification(
            webhook_url=webhook_url,
            session_id="test_webhook",
            status="completed",
            result=result
        ))
        
        # Verify webhook was called
        assert mock_post.called
        call_args = mock_post.call_args
        
        # Verify URL
        assert call_args[0][0] == webhook_url
        
        # Verify payload
        payload = call_args[1]["json"]
        assert payload["session_id"] == "test_webhook"
        assert payload["status"] == "completed"
        assert payload["files_analyzed"] == 5
        assert payload["total_issues"] == 2
        assert payload["quality_score"] == 92.0


def test_e2e_webhook_notification_on_issues_found(coordinator, small_codebase):
    """
    Test webhook notification when issues are found.
    
    Validates:
    - Webhook triggered when issues detected
    - Issue information in payload
    """
    import asyncio
    from api.main import send_webhook_notification
    from models.data_models import (
        AnalysisResult, Documentation, MetricsSummary,
        FileAnalysis, CodeMetrics, CodeIssue, IssueSeverity, IssueCategory
    )
    
    webhook_url = "https://example.com/webhook/issues"
    
    with patch('httpx.AsyncClient.post') as mock_post:
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response
        
        # Create result with issues
        file_analysis = FileAnalysis(
            file_path="test.py",
            language="python",
            metrics=CodeMetrics(
                cyclomatic_complexity=10,
                maintainability_index=60.0,
                lines_of_code=100,
                comment_ratio=0.1
            ),
            issues=[
                CodeIssue(
                    severity=IssueSeverity.HIGH,
                    category=IssueCategory.COMPLEXITY,
                    file_path="test.py",
                    line_number=10,
                    description="High complexity",
                    code_snippet="def complex():"
                ),
                CodeIssue(
                    severity=IssueSeverity.CRITICAL,
                    category=IssueCategory.SECURITY,
                    file_path="test.py",
                    line_number=20,
                    description="SQL injection risk",
                    code_snippet="query = f'SELECT * FROM users WHERE id={user_id}'"
                )
            ],
            functions=[],
            classes=[]
        )
        
        result = AnalysisResult(
            session_id="test_webhook_issues",
            timestamp=datetime.now(timezone.utc),
            codebase_path=small_codebase,
            files_analyzed=1,
            total_issues=2,
            quality_score=65.0,
            file_analyses=[file_analysis],
            suggestions=[],
            documentation=Documentation(
                project_structure="Test",
                api_docs={},
                examples={}
            ),
            metrics_summary=MetricsSummary(
                total_files=1,
                total_lines=100,
                average_complexity=10.0,
                average_maintainability=60.0,
                total_issues_by_severity={IssueSeverity.HIGH: 1, IssueSeverity.CRITICAL: 1},
                total_issues_by_category={IssueCategory.COMPLEXITY: 1, IssueCategory.SECURITY: 1}
            )
        )
        
        # Send webhook (run async function in sync context)
        asyncio.run(send_webhook_notification(
            webhook_url=webhook_url,
            session_id="test_webhook_issues",
            status="completed",
            result=result
        ))
        
        # Verify webhook was called
        assert mock_post.called
        
        # Verify payload includes issue information
        payload = mock_post.call_args[1]["json"]
        assert payload["total_issues"] == 2
        assert payload["quality_score"] == 65.0


# ============================================================================
# End-to-End Tests: CI/CD Integration
# ============================================================================

def test_e2e_cicd_pr_mode_analysis(coordinator, temp_dir):
    """
    Test CI/CD integration with PR mode (incremental analysis).
    
    Validates:
    - Changed file detection
    - Incremental analysis
    - CI/CD-friendly output
    """
    # Create a git-like scenario with changed files
    codebase_path = Path(temp_dir) / "cicd_codebase"
    codebase_path.mkdir()
    
    # Create initial files
    (codebase_path / "unchanged.py").write_text("def old_function(): pass")
    (codebase_path / "modified.py").write_text("def original(): pass")
    (codebase_path / "new.py").write_text("def new_function(): pass")
    
    # Simulate PR with changed files
    changed_files = [
        str(codebase_path / "modified.py"),
        str(codebase_path / "new.py")
    ]
    
    # Analyze only changed files
    config = AnalysisConfig(
        target_path=str(codebase_path),
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK
    )
    
    result = coordinator.analyze_codebase(config)
    
    # In a real PR mode, we would filter to only changed files
    # For this test, verify the system can handle the workflow
    assert result is not None
    assert result.files_analyzed >= 2  # At least the changed files
    
    # Verify CI/CD-friendly output format
    json_output = result.model_dump(mode='json')
    assert isinstance(json_output, dict)
    assert "session_id" in json_output
    assert "quality_score" in json_output
    assert "files_analyzed" in json_output


def test_e2e_cicd_exit_code_on_critical_issues(coordinator, temp_dir):
    """
    Test CI/CD integration with exit code handling.
    
    Validates:
    - Critical issue detection
    - Exit code determination
    - CI/CD failure triggering
    """
    from tools.cicd_integration import ExitCodeHandler
    from models.data_models import (
        AnalysisResult, FileAnalysis, CodeMetrics, CodeIssue,
        IssueSeverity, IssueCategory, Documentation, MetricsSummary
    )
    
    exit_handler = ExitCodeHandler()
    
    # Create result with critical issues
    file_analysis = FileAnalysis(
        file_path="critical.py",
        language="python",
        metrics=CodeMetrics(
            cyclomatic_complexity=5,
            maintainability_index=70.0,
            lines_of_code=50,
            comment_ratio=0.1
        ),
        issues=[
            CodeIssue(
                severity=IssueSeverity.CRITICAL,
                category=IssueCategory.SECURITY,
                file_path="critical.py",
                line_number=10,
                description="Hardcoded credentials",
                code_snippet="password = 'admin123'"
            )
        ],
        functions=[],
        classes=[]
    )
    
    result = AnalysisResult(
        session_id="test_cicd_exit",
        timestamp=datetime.now(timezone.utc),
        codebase_path=temp_dir,
        files_analyzed=1,
        total_issues=1,
        quality_score=50.0,
        file_analyses=[file_analysis],
        suggestions=[],
        documentation=Documentation(
            project_structure="Test",
            api_docs={},
            examples={}
        ),
        metrics_summary=MetricsSummary(
            total_files=1,
            total_lines=50,
            average_complexity=5.0,
            average_maintainability=70.0,
            total_issues_by_severity={IssueSeverity.CRITICAL: 1},
            total_issues_by_category={IssueCategory.SECURITY: 1}
        )
    )
    
    # Determine exit code
    exit_code = exit_handler.get_exit_code(result, fail_on_critical=True)
    
    # Should fail (non-zero exit code) due to critical issue
    assert exit_code != 0


def test_e2e_cicd_sarif_output_format(coordinator, small_codebase):
    """
    Test CI/CD integration with SARIF output format.
    
    Validates:
    - SARIF format generation
    - GitHub/GitLab compatibility
    """
    from tools.cicd_integration import OutputFormatter
    
    formatter = OutputFormatter()
    
    # Run analysis
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    result = coordinator.analyze_codebase(config)
    
    # Convert to SARIF format
    sarif_json = formatter.to_sarif(result)
    sarif_output = json.loads(sarif_json)
    
    # Verify SARIF structure
    assert isinstance(sarif_output, dict)
    assert "version" in sarif_output
    assert "runs" in sarif_output
    assert isinstance(sarif_output["runs"], list)
    
    if len(sarif_output["runs"]) > 0:
        run = sarif_output["runs"][0]
        assert "tool" in run
        assert "results" in run


# ============================================================================
# End-to-End Tests: Error Handling and Recovery
# ============================================================================

def test_e2e_error_handling_invalid_files(coordinator, temp_dir):
    """
    Test error handling with invalid/unparseable files.
    
    Validates:
    - Graceful degradation
    - Partial results generation
    - Error reporting
    """
    codebase_path = Path(temp_dir) / "error_codebase"
    codebase_path.mkdir()
    
    # Create valid file
    (codebase_path / "valid.py").write_text("def valid(): pass")
    
    # Create invalid Python file
    (codebase_path / "invalid.py").write_text("def invalid( this is not valid python")
    
    # Create another valid file
    (codebase_path / "another_valid.py").write_text("def another(): pass")
    
    config = AnalysisConfig(
        target_path=str(codebase_path),
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK
    )
    
    # Run analysis - should handle errors gracefully
    result = coordinator.analyze_codebase(config)
    
    # Should have analyzed at least the valid files
    assert result is not None
    assert result.files_analyzed >= 2  # At least the 2 valid files
    
    # Should still generate a report
    report = coordinator.generate_review_report(result)
    assert len(report) > 0


def test_e2e_error_recovery_with_checkpoints(coordinator, medium_codebase):
    """
    Test error recovery using checkpoints.
    
    Validates:
    - Checkpoint creation
    - Recovery from interruption
    - Progress preservation
    """
    config = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    session_id = "test_error_recovery"
    
    # Create session
    files = coordinator.file_system.discover_files(
        medium_codebase,
        include_patterns=["*.py"],
        exclude_patterns=[]
    )
    
    session_state = coordinator.session_manager.create_session(
        session_id=session_id,
        config=config,
        pending_files=files
    )
    
    # Simulate partial processing with checkpoints
    processed_count = len(files) // 3
    processed_files = files[:processed_count]
    pending_files = files[processed_count:]
    
    from models.data_models import FileAnalysis, CodeMetrics
    mock_analyses = []
    file_mtimes = {}
    for file_path in processed_files:
        mock_analysis = FileAnalysis(
            file_path=file_path,
            language="python",
            metrics=CodeMetrics(
                cyclomatic_complexity=2,
                maintainability_index=90.0,
                lines_of_code=20,
                comment_ratio=0.2
            ),
            issues=[],
            functions=[],
            classes=[]
        )
        mock_analyses.append(mock_analysis.model_dump(mode='json'))
        # Store modification times for change detection
        try:
            mtime = coordinator.file_system.get_modification_time(file_path)
            file_mtimes[file_path] = mtime.timestamp()
        except:
            pass
    
    # Create checkpoint with modification times
    coordinator.session_manager.checkpoint(
        session_id=session_id,
        processed_files=processed_files,
        pending_files=pending_files,
        partial_results={
            'analysis_count': processed_count,
            'file_analyses': mock_analyses,
            'file_mtimes': file_mtimes
        }
    )
    
    # Pause the session
    coordinator.pause_analysis(session_id)
    
    # Simulate crash and recovery by resuming
    recovered_state = coordinator.session_manager.load_session(session_id)
    
    assert recovered_state is not None
    assert len(recovered_state.processed_files) == processed_count
    assert len(recovered_state.pending_files) == len(files) - processed_count
    
    # Resume from checkpoint
    result = coordinator.resume_analysis(session_id)
    
    # Verify completion
    assert result is not None
    assert result.files_analyzed >= processed_count  # At least the files we had before


# ============================================================================
# End-to-End Tests: Performance and Scalability
# ============================================================================

def test_e2e_performance_metrics_collection(coordinator, medium_codebase):
    """
    Test performance metrics collection during analysis.
    
    Validates:
    - Duration tracking
    - File count metrics
    - Issue count metrics
    """
    config = AnalysisConfig(
        target_path=medium_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.STANDARD
    )
    
    start_time = time.time()
    result = coordinator.analyze_codebase(config)
    duration = time.time() - start_time
    
    # Verify metrics are collected
    assert result.files_analyzed > 0
    assert result.total_issues >= 0
    assert 0 <= result.quality_score <= 100
    
    # Verify reasonable performance
    assert duration < 120, f"Analysis took {duration}s, expected < 120s"
    
    # Verify metrics summary
    assert result.metrics_summary.total_files == result.files_analyzed
    assert result.metrics_summary.total_lines > 0
    assert result.metrics_summary.average_complexity >= 0


def test_e2e_memory_efficiency_large_codebase(coordinator, large_codebase):
    """
    Test memory efficiency with large codebase.
    
    Validates:
    - Memory usage stays reasonable
    - No memory leaks
    - Successful completion
    """
    # Skip psutil check if not available, just verify completion
    try:
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        has_psutil = True
    except ImportError:
        has_psutil = False
    
    config = AnalysisConfig(
        target_path=large_codebase,
        file_patterns=["*.py"],
        analysis_depth=AnalysisDepth.QUICK,
        enable_parallel=True
    )
    
    result = coordinator.analyze_codebase(config)
    
    # Verify completion
    assert result.files_analyzed == 100
    
    if has_psutil:
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # Verify reasonable memory usage (should not exceed 500MB increase)
        assert memory_increase < 500, f"Memory increased by {memory_increase}MB, expected < 500MB"


# ============================================================================
# Summary Test
# ============================================================================

def test_e2e_complete_system_integration(coordinator, small_codebase):
    """
    Comprehensive end-to-end test covering all major features.
    
    This test validates the complete system integration including:
    - File discovery and parsing
    - Multi-agent coordination
    - Analysis execution
    - Documentation generation
    - Suggestion generation
    - Report generation
    - Session management
    - Memory Bank integration
    
    Validates: All requirements
    """
    project_id = "complete_integration_test"
    
    # Store a pattern before analysis
    pattern = ProjectPattern(
        pattern_id="test_pattern",
        project_id=project_id,
        pattern_type=PatternType.NAMING,
        description="Test naming pattern",
        examples=["example"],
        confidence=0.8,
        last_updated=datetime.now(timezone.utc)
    )
    coordinator.memory_bank.store_pattern(pattern)
    
    # Run complete analysis
    config = AnalysisConfig(
        target_path=small_codebase,
        file_patterns=["*.py"],
        exclude_patterns=["__pycache__/**"],
        coding_standards={"max_complexity": 10, "min_comment_ratio": 0.1},
        analysis_depth=AnalysisDepth.STANDARD,
        enable_parallel=True
    )
    
    session_id = "complete_integration_session"
    result = coordinator.analyze_codebase(config, session_id=session_id)
    
    # Verify all components worked
    assert result is not None
    assert result.session_id == session_id
    assert result.files_analyzed > 0
    assert 0 <= result.quality_score <= 100
    assert len(result.file_analyses) > 0
    assert result.documentation is not None
    assert result.metrics_summary is not None
    
    # Verify session was tracked
    session_state = coordinator.get_analysis_status(session_id)
    assert session_state is not None
    assert session_state.status == SessionStatus.COMPLETED
    
    # Verify Memory Bank pattern is still accessible
    retrieved_patterns = coordinator.memory_bank.retrieve_patterns(project_id)
    assert len(retrieved_patterns) > 0
    assert any(p.pattern_id == "test_pattern" for p in retrieved_patterns)
    
    # Verify report generation
    report = coordinator.generate_review_report(result)
    assert len(report) > 0
    assert "code review" in report.lower() or "analysis" in report.lower()
    
    # Verify all file analyses have required data
    for fa in result.file_analyses:
        assert fa.file_path is not None
        assert fa.language is not None
        assert fa.metrics is not None
        assert fa.metrics.cyclomatic_complexity >= 0
        assert 0 <= fa.metrics.maintainability_index <= 100
    
    print(f"\nâœ“ Complete system integration test passed!")
    print(f"  - Files analyzed: {result.files_analyzed}")
    print(f"  - Quality score: {result.quality_score:.1f}")
    print(f"  - Total issues: {result.total_issues}")
    # Handle both SessionStatus enum and string values
    status_value = session_state.status.value if hasattr(session_state.status, 'value') else session_state.status
    print(f"  - Session status: {status_value}")
